{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b3d382",
   "metadata": {},
   "source": [
    "# Fine-tuning Diff2Lip on Custom Data\n",
    "\n",
    "This notebook shows how to fine-tune the mouth-region diffusion model (Diff2Lip)\n",
    "on your own paired video+audio dataset for improved lip-sync fidelity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887514ea",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b105c",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone Diff2Lip repo if not already present\n",
    "!git clone https://github.com/YuanGary/DiffusionLi.git diff2lip\n",
    "%cd diff2lip\n",
    "\n",
    "# Install dependencies\n",
    "!pip install -r requirements.txt\n",
    "!pip install torch torchvision diffusers accelerate transformers\n",
    "\n",
    "# Create a workspace directory\n",
    "!mkdir -p ../workspace/data\n",
    "!mkdir -p ../workspace/checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ebcda3",
   "metadata": {},
   "source": [
    "## 2. Prepare Your Dataset\n",
    "\n",
    "Place your training data under `../workspace/data` in this structure:\n",
    "\n",
    "```\n",
    "workspace/data/\n",
    "└── train/\n",
    "    ├── video1.mp4\n",
    "    ├── video1.wav\n",
    "    ├── video2.mp4\n",
    "    ├── video2.wav\n",
    "    └── ...\n",
    "```\n",
    "Each `videoN.mp4` should be a talking-head clip; `videoN.wav` the exact audio."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dfe7fb",
   "metadata": {},
   "source": [
    "## 3. Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be828e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import torchaudio\n",
    "\n",
    "class Diff2LipDataset(Dataset):\n",
    "    def __init__(self, data_dir, fps=25, crop_size=256):\n",
    "        self.samples = []\n",
    "        for file in os.listdir(os.path.join(data_dir, \"train\")):\n",
    "            if file.endswith(\".mp4\"):\n",
    "                vid = os.path.join(data_dir, \"train\", file)\n",
    "                wav = vid.replace(\".mp4\", \".wav\")\n",
    "                if os.path.exists(wav):\n",
    "                    self.samples.append((vid, wav))\n",
    "        self.fps = fps\n",
    "        self.crop = crop_size\n",
    "\n",
    "    def __len__(self): return len(self.samples)\n",
    "    def __getitem__(self, idx):\n",
    "        vid_path, wav_path = self.samples[idx]\n",
    "        # Read video frames\n",
    "        cap = cv2.VideoCapture(vid_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret: break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            # center-crop\n",
    "            h, w, _ = frame.shape\n",
    "            m = min(h, w)\n",
    "            y0, x0 = (h-m)//2, (w-m)//2\n",
    "            frm = frame[y0:y0+m, x0:x0+m]\n",
    "            frm = cv2.resize(frm, (self.crop, self.crop))\n",
    "            frames.append(frm)\n",
    "        cap.release()\n",
    "        # Load audio later in collate\n",
    "        return {\"frames\": torch.tensor(np.array(frames)).permute(0,3,1,2)/255., \"audio\": wav_path}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    vids = [x[\"frames\"] for x in batch]\n",
    "    auds = [x[\"audio\"] for x in batch]\n",
    "    # Pad video sequences\n",
    "    padded_vids = torch.nn.utils.rnn.pad_sequence(vids, batch_first=True)\n",
    "    return padded_vids, auds\n",
    "\n",
    "dataset = Diff2LipDataset(\"../workspace/data\")\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67516c48",
   "metadata": {},
   "source": [
    "## 4. Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dccda64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diff2lip.diff2lip import Diff2Lip\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = Diff2Lip(device=device).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff040357",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39426742",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (video_batch, audio_paths) in enumerate(loader):\n",
    "        video_batch = video_batch.to(device)  # shape [B,T,3,H,W]\n",
    "        # For each exemplar take first frame as “driving”\n",
    "        driving_frame = video_batch[:,0]\n",
    "        \n",
    "        # Load corresponding audio into tensor\n",
    "        aud_tensors = []\n",
    "        for ap in audio_paths:\n",
    "            wav, sr = torchaudio.load(ap)\n",
    "            aud_tensors.append(wav.mean(0))  # mono\n",
    "        aud_batch = torch.nn.utils.rnn.pad_sequence(aud_tensors, batch_first=True).to(device)\n",
    "\n",
    "        loss = model.training_step(driving_frame, aud_batch, video_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i+1}/{len(loader)}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3e066b",
   "metadata": {},
   "source": [
    "## 6. Save Fine-tuned Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4b47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"../workspace/checkpoints\", exist_ok=True)\n",
    "save_path = \"../workspace/checkpoints/diff2lip_finetuned.pth\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edb4c10",
   "metadata": {},
   "source": [
    "## 7. Inference with Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bef756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your fine-tuned weights\n",
    "model.load_state_dict(torch.load(\"../workspace/checkpoints/diff2lip_finetuned.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Use same inference API as before\n",
    "# Note: The original 'render_diff2lip' function is not exposed in the provided code.\n",
    "# We will assume the model object has a 'render' method for this example.\n",
    "try:\n",
    "    model.render(\n",
    "        video_in=\"../output_fomm.mp4\", # Assuming output from previous notebook\n",
    "        audio_in=\"../assets/hello.wav\", # Assuming assets from previous notebook\n",
    "        video_out=\"../output_diff2lip_finetuned.mp4\",\n",
    "        upscale_factor=1\n",
    "    )\n",
    "\n",
    "    from IPython.display import display, HTML\n",
    "    from base64 import b64encode\n",
    "    mp4 = open(\"../output_diff2lip_finetuned.mp4\",'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    display(HTML(f'<video controls width=480 src=\"{data_url}\">'))\n",
    "except AttributeError:\n",
    "    print(\"Inference function 'render' not found in the Diff2Lip model object.\")\n",
    "    print(\"Please adapt this cell to the correct inference API for your fine-tuned model.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6abad6",
   "metadata": {},
   "source": [
    "You’ve now fine-tuned Diff2Lip on your own dataset for personalized, high-fidelity lip syncing. Integrate these checkpoints into your Avatar Renderer Pod pipeline for production-grade avatar videos!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
