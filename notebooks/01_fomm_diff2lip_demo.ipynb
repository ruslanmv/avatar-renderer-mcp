{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First‑Order Motion Model + Diff2Lip Demo\n",
    "\n",
    "This notebook demonstrates a two‑stage talking‑head pipeline:\n",
    "\n",
    "1. **FOMM** drives full head + upper‑body motion from a reference video.  \n",
    "2. **Diff2Lip** replaces the mouth region with diffusion‐based visemes for ultra‑high‑fidelity lip sync."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repos & install dependencies (once per machine)\n",
    "!git clone https://github.com/AliaksandrSiarohin/first-order-model.git fomm\n",
    "!git clone https://github.com/YuanGary/DiffusionLi.git diff2lip\n",
    "!pip install -r fomm/requirements.txt\n",
    "!pip install -r diff2lip/requirements.txt\n",
    "!pip install torch torchvision diffusers accelerate transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Imports & Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, torch, cv2, numpy as np\n",
    "from fomm.demo import load_checkpoints, make_animation\n",
    "from diff2lip.diff2lip import Diff2Lip\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import imageio\n",
    "\n",
    "# Utility to show video inline\n",
    "def show_video(path, width=320):\n",
    "    mp4 = open(path,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(f'<video width={width} controls src=\"{data_url}\">')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1 FOMM: generator & keypoint detector\n",
    "fomm_config = \"fomm/config/vox-256.yaml\"\n",
    "fomm_checkpoint = \"fomm/checkpoints/first_order_model.pth\" # Make sure to download this checkpoint\n",
    "generator, kp_detector = load_checkpoints(fomm_config, fomm_checkpoint, device='cuda')\n",
    "\n",
    "# 3.2 Diff2Lip\n",
    "diff2lip_model = Diff2Lip(\n",
    "    vpn_model=\"diff2lip/pretrained/vpn.pth\",      # if provided\n",
    "    diff_model=\"diff2lip/pretrained/diff2lips.pth\",\n",
    "    device=\"cuda\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Still source image (one frame)\n",
    "source_image = cv2.imread(\"assets/alice.png\")[..., ::-1]  # BGR→RGB\n",
    "\n",
    "# Driving video (short clip 5-10s)\n",
    "driving_video = [ \n",
    "    cv2.imread(f\"assets/driver/{i:03d}.png\")[..., ::-1] \n",
    "    for i in range(30) \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 1 – FOMM Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run FOMM to produce initial talking-head clip\n",
    "with torch.no_grad():\n",
    "    fomm_result = make_animation(\n",
    "        source_image, driving_video,\n",
    "        generator, kp_detector,\n",
    "        relative=True, adapt_movement_scale=True,\n",
    "        device='cuda'\n",
    "    )\n",
    "\n",
    "# Save interim video\n",
    "fomm_path = \"output_fomm.mp4\"\n",
    "writer = imageio.get_writer(fomm_path, fps=10)\n",
    "for frame in fomm_result:\n",
    "    writer.append_data(frame[..., ::-1])  # RGB→BGR for imageio\n",
    "writer.close()\n",
    "\n",
    "show_video(fomm_path, width=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stage 2 – Diff2Lip Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff2Lip expects a video file + WAV audio\n",
    "audio_file = \"assets/hello.wav\"\n",
    "\n",
    "# Run Diff2Lip to replace mouth region\n",
    "refined_path = \"output_diff2lip.mp4\"\n",
    "diff2lip_model.render(\n",
    "    video_in=fomm_path,\n",
    "    audio_in=audio_file,\n",
    "    video_out=refined_path,\n",
    "    upscale_factor=1\n",
    ")\n",
    "\n",
    "show_video(refined_path, width=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a high‑fidelity talking‑head MP4:\n",
    "\n",
    "- Full head motion and expressions from FOMM.\n",
    "- Diffusion‑quality lip sync from Diff2Lip.\n",
    "\n",
    "Integrate this pipeline into your Avatar Renderer Pod for production‑grade avatar videos!\n",
    "\n",
    "### Next steps:\n",
    "\n",
    "- Swap in different driver videos for style transfer.\n",
    "- Fine‑tune checkpoint on your own face dataset.\n",
    "- Hook into the MCP‑server (`mcp_server.py`) for orchestration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
