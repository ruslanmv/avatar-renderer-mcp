# Avatarâ€¯Rendererâ€¯Pod â€“ VideoGenieâ€‘class avatar generator (MCPâ€‘ready)

> A singleâ€‘imageâ€¯â†’â€¯talkingâ€‘head engine that reaches high-quality while **natively speaking the MCP protocol**.  
> Drop the container into your GPU pool and an MCP Gateway autoâ€‘discovers the `render_avatar` tool the moment the process starts.

[![PythonÂ 3.11+](https://img.shields.io/badge/python-3.11%2B-blue)]()
[![LicenseÂ Apacheâ€‘2.0](https://img.shields.io/badge/license-Apache%202.0-blue)]()

---

## ğŸš€â€¯Features

* **FOMMâ€¯(head pose)**Â +Â **Diff2Lipâ€¯(diffusion visemes)** with automatic fallback to **SadTalkerâ€¯(+â€¯Wav2Lip)** when VRAM is tight.  
* **MCP STDIO server** *and* FastAPI REST faÃ§ade live in the same container.  
* CUDAâ€¯12.4Â +Â PyTorchâ€¯2.3; NVENC H.264 encodes >â€¯200â€¯fps on a V100.  
* Pluggable pipeline (`pipeline.py`) â€“ swap in AnimateDiff, DreamTalk, LIAONâ€‘LipSync, etc.  
* Helm chart & raw manifests request **`nvidia.com/gpu: 1`** and tolerate the **`dedicated=gpu`** taint.  
* KEDAâ€‘ready: ScaledObject samples Kafka lag and spins 0â€¯â†’â€¯N render pods as demand changes.  
* Full CI (CPUâ€‘only) plus Colab notebooks for checkpoint tuning.

---

## ğŸ“¦â€¯Project tree

```text
avatar-renderer-pod/
â”œâ”€â”€ README.md                    # You are here
â”œâ”€â”€ LICENSE                      # Apacheâ€‘2.0
â”‚
â”œâ”€â”€ app/                         # Runtime code
â”‚   â”œâ”€â”€ api.py                   # FastAPI â†’ Celery task
â”‚   â”œâ”€â”€ mcp_server.py            # STDIO MCP server (render_avatar)
â”‚   â”œâ”€â”€ worker.py                # Celery worker bootstrap
â”‚   â”œâ”€â”€ pipeline.py              # FOMM+Diff2Lip â†’ FFmpeg glue
â”‚   â”œâ”€â”€ viseme_align.py          # Montreal Forced Aligner helper
â”‚   â””â”€â”€ settings.py              # Pydantic env config
â”‚
â”œâ”€â”€ models/                      # ğŸ”¹ **not tracked** â€“ mount at runtime
â”‚   â”œâ”€â”€ fomm/                    # `vox-cpk.pth` â€“Â FOMM (Aliaksandrâ€¯Siarohin, 2020)
â”‚   â”œâ”€â”€ diff2lip/                # `Diff2Lip.pth` â€“ diffusion visemes (Yuanâ€¯Gary, 2024)
â”‚   â”œâ”€â”€ sadtalker/               # `sadtalker.pth` â€“ SadTalker motion (Zhangâ€¯etâ€¯al.,â€¯CVPRâ€¯2023)
â”‚   â”œâ”€â”€ wav2lip/                 # `wav2lip_gan.pth` â€“ lip GAN (KÂ Rudrabha, 2020)
â”‚   â””â”€â”€ gfpgan/                  # `GFPGANv1.3.pth` â€“ face enhancer (Tencentâ€¯ARC, 2021)
â”‚
â”œâ”€â”€ Dockerfile                   # CUDAÂ 12.4 runtime image
â”œâ”€â”€ requirements.txt             # torch, diffusers, fastapi, celery â€¦
â”‚
â”œâ”€â”€ charts/                      # Helm deployment & service
â”‚   â””â”€â”€ avatar-renderer/
â”‚       â””â”€â”€ values.yaml
â”‚
â”œâ”€â”€ k8s/                         # Raw YAML (if Helm not used)
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ autoscale.yaml
â”‚
â”œâ”€â”€ mcp-tool.json                # Manifest autoâ€‘discovered by Gateway
â”‚
â”œâ”€â”€ ci/
â”‚   â”œâ”€â”€ github-actions.yml       # lint â†’ build â†’ test â†’ push
â”‚   â””â”€â”€ tekton-build.yaml
â”‚
â”œâ”€â”€ notebooks/                   # Colab/Jupyter demos & fineâ€‘tune
â”‚   â”œâ”€â”€ 01_fomm_diff2lip_demo.ipynb
â”‚   â””â”€â”€ 02_finetune_diff2lip.ipynb
â”‚
â”œâ”€â”€ scripts/                     # Utility helpers
â”‚   â”œâ”€â”€ download_models.sh       # Fetch all checkpoints (â‰ˆÂ 3â€¯GB)
â”‚   â”œâ”€â”€ benchmark.py             # FPS & latency profiler
â”‚   â””â”€â”€ healthcheck.sh           # Curlâ€‘based liveness probe
â”‚
â”œâ”€â”€ tests/                       # pytest (CPU) smoke â‰¤â€¯5â€¯s
â”‚   â”œâ”€â”€ conftest.py
â”‚   â”œâ”€â”€ test_render_api.py
â”‚   â”œâ”€â”€ test_mcp_stdio.py
â”‚   â””â”€â”€ assets/ {alice.png, hello.wav}
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ 00_overview.md
    â””â”€â”€ 02_mcp_integration.md
````

---

## ğŸ–¼Â Render workflow (detailed)

```mermaid
sequenceDiagram
    autonumber
    participant API as FastAPI / MCPÂ STDIO
    participant Celery as CeleryÂ Worker
    participant FOMM as FOMMÂ (Headâ€¯+â€¯UpperÂ Body)
    participant Diff2Lip as Diff2LipÂ (DiffusionÂ Visemes)
    participant SadTalker as SadTalkerÂ (MotionÂ Fallback)
    participant W2L as Wav2LipÂ (LipÂ GANÂ Fallback)
    participant FFmpeg as FFmpegÂ NVENC

    API->>API: POST /render (avatarId, voiceUrl)
    API->>Celery: enqueue(job)
    opt reference_video provided
        Celery->>FOMM: fullâ€‘motion driving video
    else
        Celery->>FOMM: infer pose from still + audio
    end
    alt GPU RAM >=â€¯12â€¯GB
        Celery->>Diff2Lip: diffuse mouth frames
        Diff2Lip-->>Celery: 25â€¯fps RGBA frames
    else
        Celery->>SadTalker: generate 3D coefficients
        SadTalker-->>Celery: coarse frames
        Celery->>W2L: refine lips (GAN)
        W2L-->>Celery: final frames
    end
    Celery->>FFmpeg: encode H.264 (h264_nvenc)
    FFmpeg-->>Celery: out.mp4
    Celery-->>API: signed COS URL
```

* **FOMM** or **SadTalker** provides realistic head pose, eyeâ€‘blink and basic expression.
* **Diff2Lip** (Stableâ€¯Diffusion inâ€‘painting) improves lip realism; falls back to **Wav2Lip** when VRAM is scarce.
* Final MP4 is encoded with `-profile:v high -preset p7 -b:v 6M` for presentationâ€‘ready quality.

---

## ğŸ› Â Local quickâ€‘start (GPU workstation)

```bash
# 1. Clone & pull weights (â‰ˆÂ 3â€¯GB, once)
bash scripts/download_models.sh

# 2. Build & run
docker build -t avatar-renderer:dev .
docker run --gpus all -p 8080:8080 \
  -v $(pwd)/models:/models avatar-renderer:dev &

# 3. Render
curl -X POST localhost:8080/render \
  -H 'Content-Type: application/json' \
  -d '{"avatarId":"alice","voiceUrl":"https://example.com/hello.wav"}'
```

---

## ğŸ”—Â MCP integration

```bash
curl -X POST http://gateway:4444/servers \
 -H "Authorization: Bearer $ADMIN_TOKEN" \
 -H "Content-Type: application/json" \
 -d '{
   "name": "avatar-renderer",
   "transport": "stdio",
   "command": "/usr/bin/python3",
   "args": ["/app/mcp_server.py"],
   "autoDiscover": true
 }'
```

Gateway detects `mcp-tool.json` and registers the **`render_avatar`** tool automatically.

---

## ğŸš¦Â Feature compliance matrix

| Feature                                     | Status  | Notes                                              |
| ------------------------------------------- | ------- | -------------------------------------------------- |
| Realistic facial animation from still image | **âœ…**   | FOMMâ€¯/â€¯SadTalker for full head + expressions       |
| Highâ€‘fidelity lipâ€‘sync                      | **âœ…**   | Diff2Lip diffusion visemes or Wav2Lip GAN fallback |
| MP4 export (presentationâ€‘ready)             | **âœ…**   | H.264 NVENC, signed COS/S3 URL                     |
| Live WebRTC streaming                       | âš ï¸Â Soon | GStreamer NVENC RTP branch in `feature/webrtc`     |
| AIâ€‘agent integration (MCP)                  | **âœ…**   | STDIO protocol ready, REST remains for demos       |
| Lowâ€‘latency incremental synthesis           | âš ï¸Â R\&D | Needs chunked TTS + slidingâ€‘window Diff2Lip        |

---

## âš™Â Helm deployment (OpenShift)

```bash
helm upgrade --install avatar-renderer charts/avatar-renderer \
  --namespace videogenie --create-namespace \
  --set image.tag=$(git rev-parse --short HEAD)
```

Requests **1â€¯GPU**, 6â€¯GiB RAM, 2Â vCPU. The existing VideoGenie KEDA ScaledObject will autoscale pods based on Kafka lag.


# Makefile Guide

```bash
# firstâ€‘time developer workflow
make setup
make download-models
make run        # REST server â†’ http://localhost:8080/render

# MCP stdio test
make run-stdio  # then echo '{"tool":"render_avatar", ...}' | ./app/mcp_server.py

# build + run container
make docker-build
make docker-run
```

---

## ğŸ›Â Troubleshooting

| Symptom                                 | Fix / Hint                                                         |
| --------------------------------------- | ------------------------------------------------------------------ |
| `CUDA error: out of memory`             | Reduce `--diff2lip_steps`, enable Wav2Lip fallback, or upgrade GPU |
| Stuck at â€œAlign visemes â€¦â€              | Ensure M.F.Â Aligner English model in `models/mfa/`                 |
| Green / black artefacts in output video | Driver â‰¥â€¯545, verify FFmpeg built with `--enable-nvenc`            |
| Lips drift from audio by >â€¯100â€¯ms       | Check `viseme_align.py` phoneme timing; reâ€‘sample audioÂ 16â€¯kHz     |

---

## ğŸ“œÂ License

ApacheÂ 2.0 â€” use it, fork it, break it, fix it, **PR back** ğŸ™Œ

